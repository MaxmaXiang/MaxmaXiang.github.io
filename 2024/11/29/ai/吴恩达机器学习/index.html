<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习 | 马翔的博客</title><meta name="author" content="马翔"><meta name="copyright" content="马翔"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习机器学习的应用：语音识别，计算机视觉，增强现实，自动驾驶，大规模农业，医疗保健 机器学习的分类： 有监督学习：回归，分类   无监督学习：聚类  监督学习从x到y的预测,函数的复杂版  电子邮件-&gt;垃圾邮件?(0&#x2F;1)    垃圾邮件过滤 音频-&gt;文本转录    语音识别 英语-&gt;西班牙语    机器翻译  广告-&gt;用户信息 点击?(0&#x2F;1)    在线广告  图像">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="https://maxmaxiang.github.io/2024/11/29/ai/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="马翔的博客">
<meta property="og:description" content="机器学习机器学习的应用：语音识别，计算机视觉，增强现实，自动驾驶，大规模农业，医疗保健 机器学习的分类： 有监督学习：回归，分类   无监督学习：聚类  监督学习从x到y的预测,函数的复杂版  电子邮件-&gt;垃圾邮件?(0&#x2F;1)    垃圾邮件过滤 音频-&gt;文本转录    语音识别 英语-&gt;西班牙语    机器翻译  广告-&gt;用户信息 点击?(0&#x2F;1)    在线广告  图像">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6472fbeff024cca1736849a1.jpg">
<meta property="article:published_time" content="2024-11-29T02:01:17.000Z">
<meta property="article:modified_time" content="2024-12-13T09:28:42.000Z">
<meta property="article:author" content="马翔">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.imgdb.cn/item/6472fbeff024cca1736849a1.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "机器学习",
  "url": "https://maxmaxiang.github.io/2024/11/29/ai/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/",
  "image": "https://pic1.imgdb.cn/item/6472fbeff024cca1736849a1.jpg",
  "datePublished": "2024-11-29T02:01:17.000Z",
  "dateModified": "2024-12-13T09:28:42.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "马翔",
      "url": "https://maxmaxiang.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="https://pic1.imgdb.cn/item/6472fc0df024cca17368939b.jpg"><link rel="canonical" href="https://maxmaxiang.github.io/2024/11/29/ai/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="马翔的博客" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://pic1.imgdb.cn/item/6455104b0d2dde5777a4fbce.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">马翔的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">机器学习</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-29T02:01:17.000Z" title="发表于 2024-11-29 10:01:17">2024-11-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-13T09:28:42.000Z" title="更新于 2024-12-13 17:28:42">2024-12-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="机器学习的应用："><a href="#机器学习的应用：" class="headerlink" title="机器学习的应用："></a>机器学习的应用：</h2><p>语音识别，计算机视觉，增强现实，自动驾驶，大规模农业，医疗保健</p>
<h2 id="机器学习的分类："><a href="#机器学习的分类：" class="headerlink" title="机器学习的分类："></a>机器学习的分类：</h2><ul>
<li>有监督学习：回归，分类</li>
</ul>
<ul>
<li>无监督学习：聚类</li>
</ul>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>从x到y的预测,函数的复杂版</p>
<ul>
<li>电子邮件-&gt;垃圾邮件?(0/1)    垃圾邮件过滤</li>
<li>音频-&gt;文本转录    语音识别</li>
<li><p>英语-&gt;西班牙语    机器翻译</p>
</li>
<li><p>广告-&gt;用户信息 点击?(0/1)    在线广告</p>
</li>
<li><p>图像，雷达信息-&gt;其他车的位置    自动驾驶汽车</p>
</li>
</ul>
<h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>最简单的回归是用一元一次线性函数预测，根据已有的点来拟合出一个一元一次线性函数，然后用线性函数预测其他点。也可以将目标函数从y=ax+b替换成更复杂的其他函数，比如二次函数等。</p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>分类只输出一小部分的可能或2元类别（是否/对错）。即值域是不连续的，取值范围是有限多个值。常用的简单的函数模型有$\frac{1}{1+e^{x} }  $。</p>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>无监督学习不需要从x到y的映射。输入为有一系列特征的数据集，如：(年龄,肿瘤大小)：（23，3）、（35，6）、（60，10）.无监督学习的目标是从数据集中寻找数据的结构和模式。</p>
<p>使用聚类算法对dna进行分类，将不同的人分成不同类别。</p>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>异常检测：检测系统中的异常数据，比如金融系统的欺诈操作流水。</p>
<p>降维：将大数据压缩成小得多的数据集并尽可能少丢失信息。</p>
<h2 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_62327883/article/details/135672680">从零开始！Jupyter Notebook的安装教程（附带pip和Python的安装教程）_jupyter notebook安装教程-CSDN博客</a></p>
<h2 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h2><p>线性回归是最简单的监督学习模型。我们假设只有一个影响因素，即特征的维度是1.所以模型函数如下：</p>
<p>f(w,b)(x)=wx+b</p>
<p>给定一组关于（x,y）的坐标，显然两点确定一条直线，当坐标的数量大于2时，不止一条直线可以选择。此时如何确定w和b？</p>
<p>使用损失函数（loss）（代价函数）（cost function）：</p>
<p>常见的损失函数有：平方误差成本函数</p>
<script type="math/tex; mode=display">
\frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{i}-y^{i} )^{2}</script><p>然后，我们的目标就是最小化损失函数。当我们把所有点代入损失函数后，实际上得到了一个关于w和b的二次函数。</p>
<p>我们假设这组坐标是(x1,y1),(x2,y2)….(xm,ym)，一共m个点。那原式=</p>
<script type="math/tex; mode=display">
\frac{1}{2m} \sum_{i=1}^{m} (y_i-(wx_i+b) )^{2}=\frac{1}{2m} \sum_{i=1}^{m} (-y_i^2+(w^2x_i^2+b^2+2x_iwb) -2wx_iy_i-2by_i)</script><p>如果我们先把b当做常数，求使得loss函数最小的w值，那这就是关于w的一个开口向上的二次函数，对称轴是-b/2a，即</p>
<script type="math/tex; mode=display">
\frac{1}{2m} \sum_{i=1}^{m} ((w^2x_i^2+2x_iwb) -2wx_iy_i)=\frac{1}{2m} \sum_{i=1}^{m} (x_i^2)*w^2+\frac{1}{2m} \sum_{i=1}^{m} (2x_ib -2x_iy_i)*w</script><script type="math/tex; mode=display">
a=\frac{1}{2m} \sum_{i=1}^{m} (x_i^2) \\
b=\frac{1}{2m} \sum_{i=1}^{m} (2x_ib -2x_iy_i)\\
\frac{-b}{2a}=\frac{\sum_{i=1}^{m} ( x_iy_i-x_ib)}{\sum_{i=1}^{m} (x_i^2)}</script><p>对于(1,1),(2,2),(3,3).我们假设b=0.那么关于w的二次函数的对称轴：</p>
<script type="math/tex; mode=display">
\frac{-b}{2a}=\frac{\sum_{i=1}^{m} ( x_iy_i)}{\sum_{i=1}^{m} (x_i^2)}=\frac{1*1+2*2+3*3}{1^2+2^2+3^2}=\frac{14}{14}=1</script><p>很明显，对于(1,1),(2,2),(3,3)的原函数是y=x,即w=1,b=0，刚好对应上。</p>
<p>如果考虑到b，即推广到三维。</p>
<p>我们将原式化简，我们得到：</p>
<script type="math/tex; mode=display">
f(w, b) = \frac{1}{2m} \left( \sum_{i=1}^{m} (w x_i - y_i)^2 + \sum_{i=1}^{m} (b - y_i)^2 \right) + C</script><p>现在，我们分析函数 f(w,b) 的图像。函数 f(w,b) 是两个平方项的和，每个平方项都是关于 w 和 b的二次函数。因此，f(w,b)是一个二次函数，其图像在三维空间中是一个抛物面。</p>
<p>具体来说，由于两个平方项都是非负的，函数 f(w,b)有一个最小值，当且仅当两个平方项都为零时取得。</p>
<script type="math/tex; mode=display">
这些值可以通过代入 ( f(w, b) ) 来计算最小值。最小值是：

[ f\left( \frac{\sum_{i=1}^{m} x_i y_i}{\sum_{i=1}^{m} x_i^2}, \frac{\sum_{i=1}^{m} y_i}{m} \right) ]\\

其中，

[ w = \frac{\sum_{i=1}^{m} x_i y_i}{\sum_{i=1}^{m} x_i^2}, \quad b = \frac{\sum_{i=1}^{m} y_i}{m} ]</script><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h4 id="梯度下降的基本流程"><a href="#梯度下降的基本流程" class="headerlink" title="梯度下降的基本流程"></a>梯度下降的基本流程</h4><p>初始化参数：<br>选择一个初始点 $ \theta<em>0 $ 作为参数向量的起始值。通常可以随机初始化或设置为零向量。<br>选择一个合适的学习率 $ \alpha $，它决定了每次迭代时参数更新的步长。学习率的选择非常重要，过大可能导致算法不收敛，过小则可能导致收敛速度过慢。<br>计算损失函数的梯度：<br>对于给定的参数 $ \theta $，计算损失函数 $ J(\theta) $ 关于每个参数的偏导数，即梯度 $ \nabla</em>\theta J(\theta) $。梯度指出了损失函数在当前参数点处增长最快的方向。<br>更新参数：<br>根据梯度的负方向更新参数，因为梯度指向的是损失函数增加最快的方向，所以我们需要沿相反方向移动以减小损失函数。更新公式为： $ \theta := \theta - \alpha \nabla<em>\theta J(\theta) $<br>这里的 $ \alpha $ 是学习率，控制每次更新的步长；$ \nabla</em>\theta J(\theta) $ 是损失函数关于参数的梯度。<br>重复迭代：<br>重复上述步骤，直到满足某个停止条件。常见的停止条件包括：<br>损失函数的变化非常小，即 $ |J(\theta^{(t+1)}) - J(\theta^{(t)})| &lt; \epsilon $，其中 $ \epsilon $ 是一个小的阈值。<br>参数的变化非常小，即 $ |\theta^{(t+1)} - \theta^{(t)}| &lt; \epsilon $.<br>达到预设的最大迭代次数。<br>输出最优参数：<br>当满足停止条件时，输出最终的参数 $ \theta $，这些参数使得损失函数达到最小值或接近最小值。</p>
<h4 id="梯度下降的不同变体"><a href="#梯度下降的不同变体" class="headerlink" title="梯度下降的不同变体"></a>梯度下降的不同变体</h4><p>根据每次更新时使用的样本数量，梯度下降有以下几种常见变体：</p>
<ol>
<li>批量梯度下降（Batch Gradient Descent, BGD）<br>特点：每次迭代使用所有训练样本计算梯度，并更新参数。<br>优点：每次更新都基于全局信息，理论上可以找到全局最优解（对于凸函数）。<br>缺点：计算复杂度高，尤其是当数据集较大时，每次迭代都需要遍历整个数据集，计算时间较长。</li>
<li>随机梯度下降（Stochastic Gradient Descent, SGD）<br>特点：每次迭代只使用一个随机样本计算梯度，并更新参数。<br>优点：每次迭代的计算量小，适合处理大规模数据集；由于引入了随机性，可以在某些情况下更快地跳出局部极小值。<br>缺点：更新过程较为波动，可能会导致收敛速度较慢，且容易在接近最优解时来回振荡。</li>
<li><p>小批量梯度下降（Mini-batch Gradient Descent, MBGD）<br>特点：每次迭代使用一小批（mini-batch）样本计算梯度，并更新参数。批大小通常介于1和整个数据集之间。<br>优点：结合了批量梯度下降和随机梯度下降的优点，既能减少计算量，又能保持一定的稳定性，适合大多数实际应用。<br>缺点：需要选择合适的批大小，批大小过小可能导致波动，过大则可能增加计算时间。<br>梯度下降的改进方法<br>为了提高梯度下降的性能，研究人员提出了一些改进方法：</p>
</li>
<li><p>动量（Momentum）<br>原理：引入动量项，使得参数更新不仅依赖于当前梯度，还考虑了之前几次迭代的梯度方向。这可以帮助算法更快地穿越平坦区域，并减少振荡。<br>更新公式： $ v := \beta v - \alpha \nabla_\theta J(\theta) $ $ \theta := \theta + v $ 其中，$ v $ 是动量项，$ \beta $ 是动量系数（通常取0.9左右）。</p>
</li>
<li>Nesterov加速梯度（Nesterov Accelerated Gradient, NAG）<br>原理：NAG是对动量方法的改进，它在计算梯度时提前考虑了动量的影响，从而更准确地预测下一步的位置。<br>更新公式： $ v := \beta v - \alpha \nabla_\theta J(\theta + \beta v) $ $ \theta := \theta + v $</li>
<li>自适应学习率方法<br>Adagrad：根据每个参数的历史梯度调整学习率，使得频繁更新的参数具有较小的学习率，而较少更新的参数具有较大的学习率。<br>Adadelta：改进了Adagrad的累积梯度问题，使用滑动窗口来计算历史梯度的平方和。<br>RMSprop：类似于Adadelta，但使用指数加权平均来平滑历史梯度的平方和。<br>Adam（Adaptive Moment Estimation）：结合了动量和RMSprop的优点，使用一阶矩估计（动量）和二阶矩估计（梯度平方的平滑）来动态调整学习率。</li>
</ol>
<h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>当参数只有2个时，可以使用数学方法直接计算函数的最小值。但当参数越来越多，函数就会越来越复杂。假设原式是$y=w_1 x_1+w_2 x_2+…+w_n x_n+b$</p>
<p>。那如何求解损失函数的最小值来获得w1到wn呢？</p>
<p>给定的原式是线性回归模型： $ y = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b $ </p>
<p>其中，$ y $ 是目标变量，$ x_1, x_2, \ldots, x_n $ 是特征变量，$ w_1, w_2, \ldots, w_n $ 是权重，$ b $ 是偏置项。 </p>
<p>使用的损失函数是平方误差成本函数： $ J(w<em>1, w_2, \ldots, w_n, b) = \frac{1}{2m} \sum</em>{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 $ </p>
<p>其中，$ \hat{y}^{(i)} $ 是模型预测的值，$ y^{(i)} $ 是实际值，$ m $ 是样本数量。 </p>
<p> 关于 $ w_1, w_2, \ldots, w_n $ 的函数 对于每个权重 $ w_j $（其中 $ j = 1, 2, \ldots, n $），损失函数 $ J $ 是关于 $ w_j $ 的二次函数。</p>
<p>具体来说，对于 $ w<em>j $ 的偏导数为： $ \frac{\partial J}{\partial w_j} = \frac{1}{m} \sum</em>{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)} $ </p>
<p>找到最低点 为了找到损失函数 $ J $ 的最低点，我们需要求解所有偏导数并将其设为零，即： $ \frac{\partial J}{\partial w_j} = 0 \quad \text{for all} \quad j = 1, 2, \ldots, n $ $ \frac{\partial J}{\partial b} = 0 $ 这将给出一组线性方程，称为正规方程（Normal Equations）。</p>
<p>求解这组方程可以得到 $ w_1, w_2, \ldots, w_n $ 和 $ b $ 的最优值。 </p>
<p>最小值的存在性 由于损失函数 $ J $ 是关于 $ w_1, w_2, \ldots, w_n $ 和 $ b $ 的二次函数，且二次项系数为正，因此函数是凸的。凸函数的性质之一是它们有且只有一个最小值点。因此，通过求解偏导数并将其设为零，我们可以找到损失函数 $ J $ 的全局最小值点。 </p>
<p>总结 </p>
<p>关于 $ w_1, w_2, \ldots, w_n $ 的函数是二次函数，可以通过求解偏导数并将其设为零来找到最低点。由于损失函数是凸的，所以存在且只有一个最小值点。</p>
<h3 id="梯度下降与解析解"><a href="#梯度下降与解析解" class="headerlink" title="梯度下降与解析解"></a>梯度下降与解析解</h3><p>梯度下降（Gradient Descent）在处理大规模数据集时，通常比解析解（如正规方程）表现更好，主要原因在于计算效率、内存占用和可扩展性等方面的优势。以下是详细的解释：</p>
<h4 id="1-计算复杂度"><a href="#1-计算复杂度" class="headerlink" title="1. 计算复杂度"></a>1. <strong>计算复杂度</strong></h4><ul>
<li><p><strong>正规方程</strong>：正规方程的计算复杂度为 ( O(n^3) )，其中 ( n ) 是特征的数量。这是因为需要计算 ( X^T X ) 的逆矩阵，而求逆矩阵的复杂度是立方级别的。对于高维数据集（即特征数量较多的情况），这个复杂度会变得非常大，导致计算时间过长。</p>
</li>
<li><p><strong>梯度下降</strong>：梯度下降的计算复杂度为 ( O(mn) )，其中 ( m ) 是样本数量，( n ) 是特征数量。每次迭代中，梯度下降只需要计算代价函数的梯度，并更新参数。虽然需要多次迭代才能收敛到最优解，但每次迭代的计算量相对较小，尤其是当使用随机梯度下降（SGD）或小批量梯度下降（Mini-batch Gradient Descent）时，计算量可以进一步减少。</p>
</li>
</ul>
<h4 id="2-内存占用"><a href="#2-内存占用" class="headerlink" title="2. 内存占用"></a>2. <strong>内存占用</strong></h4><ul>
<li><p><strong>正规方程</strong>：正规方程需要存储设计矩阵 ( X ) 和其转置 ( X^T )，并且还需要存储 ( X^T X ) 这个 ( n \times n ) 的方阵。对于大规模数据集，尤其是当特征数量 ( n ) 较大时，这些矩阵的存储需求会非常大，可能超出计算机的内存限制。</p>
</li>
<li><p><strong>梯度下降</strong>：梯度下降只需要存储设计矩阵 ( X ) 和输出向量 ( y )，并且每次迭代只涉及少量的矩阵运算。因此，梯度下降的内存占用相对较小，尤其适用于处理大规模数据集。</p>
</li>
</ul>
<h4 id="3-可扩展性"><a href="#3-可扩展性" class="headerlink" title="3. 可扩展性"></a>3. <strong>可扩展性</strong></h4><ul>
<li><p><strong>正规方程</strong>：由于正规方程的计算复杂度和内存占用都随着特征数量 ( n ) 的增加而急剧增长，它在处理高维数据集时的可扩展性较差。当特征数量达到数千甚至数万时，正规方程的计算几乎不可行。</p>
</li>
<li><p><strong>梯度下降</strong>：梯度下降具有良好的可扩展性，尤其是在使用随机梯度下降（SGD）或小批量梯度下降（Mini-batch Gradient Descent）时。这两种方法每次只使用一个或少量样本进行更新，因此可以有效地处理大规模数据集，甚至是流式数据。此外，梯度下降还可以通过分布式计算框架（如Spark、Hadoop等）进行并行化，进一步提高处理大规模数据的能力。</p>
</li>
</ul>
<h4 id="4-处理多重共线性"><a href="#4-处理多重共线性" class="headerlink" title="4. 处理多重共线性"></a>4. <strong>处理多重共线性</strong></h4><ul>
<li><p><strong>正规方程</strong>：如果特征之间存在高度相关性（即多重共线性），( X^T X ) 可能会变得接近奇异矩阵，导致逆矩阵不稳定或不存在。此时，正规方程可能无法正常工作，或者得到的结果不准确。</p>
</li>
<li><p><strong>梯度下降</strong>：梯度下降对多重共线性的敏感性较低。即使特征之间存在相关性，梯度下降仍然可以通过迭代逐步逼近最优解。此外，梯度下降可以通过引入正则化项（如L2正则化或L1正则化）来缓解多重共线性问题，从而提高模型的稳定性和泛化能力。</p>
</li>
</ul>
<h4 id="5-实时更新与增量学习"><a href="#5-实时更新与增量学习" class="headerlink" title="5. 实时更新与增量学习"></a>5. <strong>实时更新与增量学习</strong></h4><ul>
<li><p><strong>正规方程</strong>：正规方程是一次性求解最优参数的方法，无法方便地进行实时更新。如果数据集发生变化（例如新增了样本或特征），必须重新计算整个模型，这在实际应用中往往是不现实的。</p>
</li>
<li><p><strong>梯度下降</strong>：梯度下降可以轻松实现增量学习，即在新数据到来时，只需基于现有模型进行少量的参数更新即可。这对于在线学习和实时系统非常重要。随机梯度下降（SGD）特别适合处理流式数据，因为它每次只使用一个样本进行更新，能够快速适应新的数据变化。</p>
</li>
</ul>
<h4 id="6-非线性模型的支持"><a href="#6-非线性模型的支持" class="headerlink" title="6. 非线性模型的支持"></a>6. <strong>非线性模型的支持</strong></h4><ul>
<li><p><strong>正规方程</strong>：正规方程主要用于线性回归模型，对于非线性模型（如神经网络、支持向量机等），无法直接应用。虽然可以通过引入多项式特征等方式将非线性问题转化为线性问题，但这会大大增加特征数量，进而增加计算复杂度和内存占用。</p>
</li>
<li><p><strong>梯度下降</strong>：梯度下降不仅可以用于线性模型，还可以广泛应用于各种非线性模型，如神经网络、逻辑回归等。对于这些复杂的模型，梯度下降仍然是最常用的优化算法之一。通过反向传播算法，梯度下降可以有效地训练深度神经网络，处理高维非线性数据。</p>
</li>
</ul>
<h4 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. <strong>总结</strong></h4><p>综上所述，梯度下降在处理大规模数据集时表现出色，主要归功于其较低的计算复杂度、较小的内存占用、良好的可扩展性以及对多重共线性和非线性模型的支持。相比之下，正规方程虽然具有理论上的精确性，但在处理大规模数据集时，计算成本过高且内存占用过大，因此在实际应用中往往不如梯度下降实用。</p>
<p>在选择优化算法时，应该根据具体的应用场景和数据规模来决定。对于小规模数据集或低维特征空间，正规方程可能是更好的选择；而对于大规模数据集或高维特征空间，梯度下降及其变体（如SGD、Mini-batch GD）通常是更优的选择。</p>
<h3 id="常见凸与非凸损失函数"><a href="#常见凸与非凸损失函数" class="headerlink" title="常见凸与非凸损失函数"></a>常见凸与非凸损失函数</h3><p>常见的凸损失函数：</p>
<p>平方误差损失（Mean Squared Error, MSE）：<br>$ J(w) = \frac{1}{2m} \sum<em>{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 $<br>这是一个典型的凸函数。它的二阶导数（Hessian矩阵）是正定的，因此它是严格凸的。平方误差损失广泛用于线性回归、最小二乘法等问题中。<br>对数损失（Log Loss / Cross-Entropy Loss）：<br>$ J(w) = -\frac{1}{m} \sum</em>{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right] $<br>适用于逻辑回归和分类问题。对数损失也是凸函数，确保了优化过程中可以找到全局最优解。<br>Huber损失：<br>Huber损失结合了平方误差损失和绝对误差损失的优点，既保持了平方误差损失在小误差时的平滑性，又避免了大误差时的敏感性。它在一定范围内是二次的，超出该范围后变为线性，因此也是凸函数。<br>L2正则化项：<br>$ R(w) = \frac{\lambda}{2} | w |^2 $<br>L2正则化项是凸的，因为它是一个二次函数。它可以与上述损失函数结合使用，形成带正则化的凸损失函数。<br>非凸损失函数的例子：</p>
<p>非线性激活函数的组合：<br>在神经网络中，如果使用非线性激活函数（如ReLU、Sigmoid等），并且网络结构较为复杂（例如深度较深），那么整体的损失函数可能是非凸的。这种情况下，损失函数可能有多个局部极小值，优化过程可能会陷入局部最优解。<br>高阶多项式损失：<br>如果损失函数包含高阶多项式项（如四次或更高次项），那么它可能是非凸的。高阶多项式可能会导致函数图像出现多个波峰和波谷，从而使得优化过程难以找到全局最优解。<br>交叉熵损失与非线性模型结合：<br>虽然交叉熵损失本身是凸的，但如果与复杂的非线性模型（如深度神经网络）结合，整体的损失函数可能会变得非凸</p>
<h3 id="批量梯度下降实现"><a href="#批量梯度下降实现" class="headerlink" title="批量梯度下降实现"></a>批量梯度下降实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xjcares.extensionservice.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BatchGradientDescent</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">double</span> learningRate;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> epochs;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">BatchGradientDescent</span><span class="params">(<span class="type">double</span> learningRate, <span class="type">int</span> epochs)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.learningRate = learningRate;</span><br><span class="line">        <span class="built_in">this</span>.epochs = epochs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">double</span>[] train(<span class="type">double</span>[] X, <span class="type">double</span>[] y) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> X.length;</span><br><span class="line">        <span class="type">double</span> <span class="variable">w</span> <span class="operator">=</span> <span class="number">0.0</span>;</span><br><span class="line">        <span class="type">double</span> <span class="variable">b</span> <span class="operator">=</span> <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">epoch</span> <span class="operator">=</span> <span class="number">0</span>; epoch &lt; epochs; epoch++) &#123;</span><br><span class="line">            <span class="type">double</span> <span class="variable">dw</span> <span class="operator">=</span> <span class="number">0.0</span>;</span><br><span class="line">            <span class="type">double</span> <span class="variable">db</span> <span class="operator">=</span> <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">                <span class="type">double</span> <span class="variable">prediction</span> <span class="operator">=</span> w * X[i] + b;</span><br><span class="line">                <span class="type">double</span> <span class="variable">error</span> <span class="operator">=</span> prediction - y[i];</span><br><span class="line">                dw += error * X[i];</span><br><span class="line">                db += error;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            w -= learningRate * (dw / n);</span><br><span class="line">            b -= learningRate * (db / n);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Optional: Print the loss for every 100 epochs</span></span><br><span class="line">            <span class="keyword">if</span> (epoch % <span class="number">100</span> == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">double</span> <span class="variable">loss</span> <span class="operator">=</span> calculateLoss(X, y, w, b);</span><br><span class="line">                System.out.println(<span class="string">&quot;Epoch &quot;</span> + epoch + <span class="string">&quot;: Loss = &quot;</span> + loss);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">double</span>[]&#123;w, b&#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">double</span> <span class="title function_">calculateLoss</span><span class="params">(<span class="type">double</span>[] X, <span class="type">double</span>[] y, <span class="type">double</span> w, <span class="type">double</span> b)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> X.length;</span><br><span class="line">        <span class="type">double</span> <span class="variable">loss</span> <span class="operator">=</span> <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="type">double</span> <span class="variable">prediction</span> <span class="operator">=</span> w * X[i] + b;</span><br><span class="line">            <span class="type">double</span> <span class="variable">error</span> <span class="operator">=</span> prediction - y[i];</span><br><span class="line">            loss += error * error;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> loss / (<span class="number">2</span> * n);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">double</span>[] X = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;</span><br><span class="line">        <span class="type">double</span>[] y = &#123;<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="type">BatchGradientDescent</span> <span class="variable">bgd</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BatchGradientDescent</span>(<span class="number">0.00001</span>, <span class="number">1000000000</span>);</span><br><span class="line">        <span class="type">double</span>[] parameters = bgd.train(X, y);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;Optimal weight (w): &quot;</span> + parameters[<span class="number">0</span>]);</span><br><span class="line">        System.out.println(<span class="string">&quot;Optimal bias (b): &quot;</span> + parameters[<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://maxmaxiang.github.io">马翔</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://maxmaxiang.github.io/2024/11/29/ai/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">https://maxmaxiang.github.io/2024/11/29/ai/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://maxmaxiang.github.io" target="_blank">马翔的博客</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="https://pic1.imgdb.cn/item/6472fbeff024cca1736849a1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/21/%E6%9D%82%E8%B0%88/%E6%9F%A5%E7%90%86%E8%8A%92%E6%A0%BC%E6%8E%A8%E8%8D%90%E7%9A%84%E7%90%86%E8%AE%BA/" title="查理芒格推荐的理论"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">查理芒格推荐的理论</div></div><div class="info-2"><div class="info-item-1">数学：复利原理，排列组合原理，费马帕斯卡系统 统计学：高斯分布 物理学：平衡，临界质量 生物学：进化论。复杂适应系统 工程学：后备系统，断裂点理论 社会科学：自我组织理论，层次进化理论。艾尔法罗预判模型 心理学：误判心理学 复利原理爱因斯坦说过这么一句： “Compound interest is the eighth wonder of the world. He who understands it, earns it … he who doesn’t … pays...</div></div></div></a><a class="pagination-related" href="/2024/11/29/%E6%9D%82%E8%B0%88/%E5%BA%B7%E6%B3%A2%E5%91%A8%E6%9C%9F/" title="康波周期"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">康波周期</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://pic1.imgdb.cn/item/6472fbeff024cca1736849a1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">马翔</div><div class="author-info-description">命运和性格是同一个概念的两个名字。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习的应用：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB%EF%BC%9A"><span class="toc-number">1.2.</span> <span class="toc-text">机器学习的分类：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.</span> <span class="toc-text">监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92"><span class="toc-number">1.3.1.</span> <span class="toc-text">回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-number">1.3.2.</span> <span class="toc-text">分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.</span> <span class="toc-text">无监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB"><span class="toc-number">1.4.1.</span> <span class="toc-text">聚类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Jupyter-Notebook"><span class="toc-number">1.5.</span> <span class="toc-text">Jupyter Notebook</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.6.</span> <span class="toc-text">线性回归模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.7.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">梯度下降的基本流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E4%B8%8D%E5%90%8C%E5%8F%98%E4%BD%93"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">梯度下降的不同变体</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">1.7.1.</span> <span class="toc-text">正规方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="toc-number">1.7.2.</span> <span class="toc-text">梯度下降与解析解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">1. 计算复杂度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">2. 内存占用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">3. 可扩展性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%A4%84%E7%90%86%E5%A4%9A%E9%87%8D%E5%85%B1%E7%BA%BF%E6%80%A7"><span class="toc-number">1.7.2.4.</span> <span class="toc-text">4. 处理多重共线性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%AE%9E%E6%97%B6%E6%9B%B4%E6%96%B0%E4%B8%8E%E5%A2%9E%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.7.2.5.</span> <span class="toc-text">5. 实时更新与增量学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%94%AF%E6%8C%81"><span class="toc-number">1.7.2.6.</span> <span class="toc-text">6. 非线性模型的支持</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-%E6%80%BB%E7%BB%93"><span class="toc-number">1.7.2.7.</span> <span class="toc-text">7. 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%87%B8%E4%B8%8E%E9%9D%9E%E5%87%B8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.7.3.</span> <span class="toc-text">常见凸与非凸损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.7.4.</span> <span class="toc-text">批量梯度下降实现</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2099/02/03/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/" title="名人名言">名人名言</a><time datetime="2099-02-03T14:44:24.000Z" title="发表于 2099-02-03 22:44:24">2099-02-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/28/%E5%88%9B%E4%B8%9A/%E5%9F%BA%E4%BA%8E%E9%95%BF%E6%9D%BF%E5%81%9A%E4%B8%9A%E5%8A%A1%EF%BC%8C%E5%9F%BA%E4%BA%8E%E7%9F%AD%E6%9D%BF%E5%81%9A%E7%BB%84%E7%BB%87/" title="基于长板做业务，基于短板做组织">基于长板做业务，基于短板做组织</a><time datetime="2025-08-28T00:01:17.000Z" title="发表于 2025-08-28 08:01:17">2025-08-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/28/%E5%88%9B%E4%B8%9A/%E7%94%A8%E4%BA%BA%E5%81%9A%E4%BA%8B%EF%BC%8C%E7%94%A8%E4%BA%8B%E5%81%9A%E4%BA%BA/" title="用人做事，用事做人">用人做事，用事做人</a><time datetime="2025-08-28T00:01:17.000Z" title="发表于 2025-08-28 08:01:17">2025-08-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/28/%E5%88%9B%E4%B8%9A/%E7%BB%8F%E8%90%A5%E4%B8%8E%E7%AE%A1%E7%90%86/" title="经营与管理">经营与管理</a><time datetime="2025-08-28T00:01:17.000Z" title="发表于 2025-08-28 08:01:17">2025-08-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/19/%E8%AF%BB%E5%90%8E%E6%84%9F/%E5%B0%B1%E4%B8%9A%E5%88%A9%E6%81%AF%E5%92%8C%E8%B4%A7%E5%B8%81%E9%80%9A%E8%AE%BA/" title="就业利息和货币通论">就业利息和货币通论</a><time datetime="2025-08-19T00:01:17.000Z" title="发表于 2025-08-19 08:01:17">2025-08-19</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By 马翔</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.0-b1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>